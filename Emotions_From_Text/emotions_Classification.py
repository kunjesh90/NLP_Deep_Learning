# -*- coding: utf-8 -*-
"""Vuclip_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hAW0X8yGKyBVg64mQjoAkIkGNT9j2cdK
"""

import numpy as np
import pandas as pd

'''
if doing it on colab than use below 2 lines 
#from google.colab import files
#uploaded = files.upload() #upload the file here

'''
import tensorflow as tf
print(tf.__version__)

tf.enable_eager_execution() #Not req if it's tf version 2.0

import os
os.chdir("C:\\Users\\kunjeshparekh\\Desktop\\KP\\IMS\\py\\project\\vu")
train=pd.read_csv("nl.csv",header=None)

train.columns=(["data","sentiment"])

train.info()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(train.iloc[:,0], train.iloc[:,1], train_size=0.8 , random_state=100)

train_np=np.array(X_train)
train_sent=np.array(y_train)
test_np=np.array(X_test)
test_sent=np.array(y_test)
print(train_np[0:2])
print(train_sent[0:2])
print(test_np[0:2])
print(test_sent[0:2])

vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type='post'
oov_tok = "<OOV>"


from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tkr = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tkr.fit_on_texts(train_np)
word_index = tkr.word_index
sequences = tkr.texts_to_sequences(train_np)
padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)

testing_sequences = tkr.texts_to_sequences(test_np)
testing_padded = pad_sequences(testing_sequences,maxlen=max_length)

np.max(padded) #9075which is less than 10000 here so vocab_size=10000 is good judgement


#for non LSTM use below
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    #tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(26, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(train_sent)), activation='softmax')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

'''
#Other way to do 

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(26, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(train_sent)), activation='softmax')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()
'''

'''
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(26, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(len(np.unique(train_sent)), activation='softmax')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()
'''
num_epochs = 10
model.fit(padded, np.array(pd.get_dummies(y_train)), epochs=num_epochs, validation_data=(testing_padded, np.array(pd.get_dummies(y_test))))

predictions = model.predict(testing_padded)
print(testing_padded.shape)
print(predictions.shape)
print((pd.get_dummies(y_test).shape))
count=0
wrong_count=0
for i in range(len(y_test)):
  if  predictions[i,:].argmax()==np.array(pd.get_dummies(y_test))[i,:].argmax():
    count+=1
  else:

    wrong_count+=1
  
print("Accuracy=",count*100/len(y_test),"%")

pd.get_dummies(y_test).columns

z=[]
for i in range(len(y_test)):
  z.append(predictions[i].argmax())
z=np.array(z)
pred=np.where(z==0,pd.get_dummies(y_test).columns[0],np.where(z==1,pd.get_dummies(y_test).columns[1],np.where(z==2,pd.get_dummies(y_test).columns[2],np.where(z==3,pd.get_dummies(y_test).columns[3],np.where(z==4,pd.get_dummies(y_test).columns[4],np.where(z==5,pd.get_dummies(y_test).columns[5],pd.get_dummies(y_test).columns[6]))))))
print((pred==y_test).value_counts())
pred#Will give you all the required predictions on the test set